{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import neccessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://github.com/ultralytics/assets/releases/download/v8.1.0/yolov8l-pose.pt to 'yolov8l-pose.pt'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 85.3M/85.3M [00:27<00:00, 3.23MB/s]\n"
     ]
    }
   ],
   "source": [
    "# use cuda if available\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "# load model\n",
    "# model = YOLO(\"yolov8x-pose-p6.pt\").to(device)\n",
    "model = YOLO(\"yolov8l-pose.pt\").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### function to calculate the angle between lines joining the joints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_angle(a, b, c):\n",
    "    \"\"\"\n",
    "    Calculate the angle between b point\n",
    "    or angle between the line ab and bc\n",
    "    \n",
    "    input:\n",
    "        a, b, c: coordinates of the keypoints/joints\n",
    "        \n",
    "    output:\n",
    "        angle: angle between the line ab and bc\n",
    "    \"\"\"\n",
    "    \n",
    "    a = np.array(a) # eg: shoulder\n",
    "    b = np.array(b) # eg: elbow\n",
    "    c = np.array(c) # eg: wrist\n",
    "    \n",
    "    # here 0 and 1 refers to the x and y coordinates\n",
    "    radians = np.arctan2(c[1]-b[1], c[0]-b[0]) - np.arctan2(a[1]-b[1], a[0]-b[0])\n",
    "    angle = np.abs(radians*180.0/np.pi)\n",
    "    \n",
    "    if angle > 180.0:\n",
    "        angle = 360 - angle\n",
    "        \n",
    "    return angle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![a](https://imgs.search.brave.com/7V4zekIn72yF8M2-JgndrVulV48qYUCsX2MvMW0eIKI/rs:fit:860:0:0/g:ce/aHR0cHM6Ly9kZWJ1/Z2dlcmNhZmUuY29t/L3dwLWNvbnRlbnQv/dXBsb2Fkcy8yMDIw/LzEwL2tleXBvaW50/X3Jjbm5fZXhtcC5q/cGc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSP\n",
    "    parameters that define the pose of an object : variable\n",
    "    variable is the set of all possible values that variable can take\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mark_keypoints(rframe, keypoints):\n",
    "    \"\"\"\n",
    "    Mark the keypoints on the image\n",
    "    \n",
    "    input:\n",
    "        keypoints: list of keypoints\n",
    "        \n",
    "    output:\n",
    "        img: image with keypoints marked\n",
    "    \"\"\"\n",
    "    # use keypoints 5 for left shoulder , 7 for left elbow, 9 for left wrist\n",
    "    shoulder, elbow, wrist = keypoints[5], keypoints[7], keypoints[9]\n",
    "    shoulder1, elbow1, wrist1 = keypoints[6], keypoints[8], keypoints[10]                \n",
    "    l11, l21, l31 = keypoints[11], keypoints[13], keypoints[15] \n",
    "    l12, l22, l32 = keypoints[12], keypoints[14], keypoints[16] \n",
    "    # get the angle\n",
    "    angle = calculate_angle(shoulder, elbow, wrist)\n",
    "    cv2.putText(rframe, f\"angle: {angle:.2f}\", (int(elbow[0]) + 10, int(elbow[1])), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "    cv2.circle(rframe, (int(shoulder[0]), int(shoulder[1])), 5, (0, 0, 255), -1)\n",
    "    cv2.circle(rframe, (int(elbow[0]), int(elbow[1])), 5, (0, 0, 255), -1)\n",
    "    cv2.circle(rframe, (int(wrist[0]), int(wrist[1])), 5, (0, 0, 255), -1)\n",
    "    cv2.line(rframe, (int(shoulder[0]), int(shoulder[1])), (int(elbow[0]), int(elbow[1])), (0, 255, 0), 2)\n",
    "    cv2.line(rframe, (int(elbow[0]), int(elbow[1])),(int(wrist[0]), int(wrist[1])) , (0, 255, 0), 2)\n",
    "    \n",
    "    # get the angle\n",
    "    angle = calculate_angle(shoulder1, elbow1, wrist1)\n",
    "    cv2.putText(rframe, f\"angle: {angle:.2f}\", (int(elbow1[0]) + 10, int(elbow1[1])), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "    cv2.circle(rframe, (int(shoulder1[0]), int(shoulder1[1])), 5, (0, 0, 255), -1)\n",
    "    cv2.circle(rframe, (int(elbow1[0]), int(elbow1[1])), 5, (0, 0, 255), -1)\n",
    "    cv2.circle(rframe, (int(wrist1[0]), int(wrist1[1])), 5, (0, 0, 255), -1)\n",
    "    cv2.line(rframe, (int(shoulder1[0]), int(shoulder1[1])), (int(elbow1[0]), int(elbow1[1])), (0, 255, 0), 2)\n",
    "    cv2.line(rframe, (int(elbow1[0]), int(elbow1[1])),(int(wrist1[0]), int(wrist1[1])) , (0, 255, 0), 2)\n",
    "    \n",
    "    angle = calculate_angle(l12, l22, l32)\n",
    "    cv2.putText(rframe, f\"angle: {angle:.2f}\", (int(l22[0]) + 10, int(l22[1])), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "    cv2.circle(rframe, (int(l12[0]), int(l12[1])), 5, (0, 0, 255), -1)\n",
    "    cv2.circle(rframe, (int(l22[0]), int(l22[1])), 5, (0, 0, 255), -1)\n",
    "    cv2.circle(rframe, (int(l32[0]), int(l32[1])), 5, (0, 0, 255), -1)\n",
    "    cv2.line(rframe, (int(l12[0]), int(l12[1])), (int(l22[0]), int(l22[1])), (0, 255, 0), 2)\n",
    "    cv2.line(rframe, (int(l22[0]), int(l22[1])),(int(l32[0]), int(l32[1])) , (0, 255, 0), 2)\n",
    "    \n",
    "    angle = calculate_angle(l11, l21, l31)\n",
    "    cv2.putText(rframe, f\"angle: {angle:.2f}\", (int(l21[0]) + 10, int(l21[1])), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "    cv2.circle(rframe, (int(l11[0]), int(l11[1])), 5, (0, 0, 255), -1)\n",
    "    cv2.circle(rframe, (int(l21[0]), int(l21[1])), 5, (0, 0, 255), -1)\n",
    "    cv2.circle(rframe, (int(l31[0]), int(l31[1])), 5, (0, 0, 255), -1)\n",
    "    cv2.line(rframe, (int(l11[0]), int(l11[1])), (int(l21[0]), int(l21[1])), (0, 255, 0), 2)\n",
    "    cv2.line(rframe, (int(l21[0]), int(l21[1])),(int(l31[0]), int(l31[1])) , (0, 255, 0), 2)\n",
    "    \n",
    "    cv2.putText(rframe, f\"{verb_detection(keypoints)}\", (10,10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verb_detection(keypoints,):\n",
    "    \"\"\"\n",
    "    Detect the verb from the keypoints\n",
    "    \n",
    "    input:\n",
    "        keypoints: list of keypoints\n",
    "        \n",
    "    output:\n",
    "        verb: detected verb\n",
    "    \"\"\"\n",
    "    # use keypoints 5 for left shoulder , 7 for left elbow, 9 for left wrist\n",
    "    ls, le, lw = keypoints[5], keypoints[7], keypoints[9]\n",
    "    rs, re, rw = keypoints[6], keypoints[8], keypoints[10]\n",
    "    \n",
    "    # get the angle\n",
    "    angle1 = calculate_angle(ls, le, lw)\n",
    "    angle2 = calculate_angle(rs, re, rw)\n",
    "    \n",
    "    if ls[0] - 10 <= rw[0] <= ls[0] + 10:\n",
    "        verb = \"right swing\" \n",
    "        print(verb)\n",
    "        T = True\n",
    "    else:\n",
    "        verb = \"swing back\"  \n",
    "        print(verb)\n",
    "    if rs[0] - 10 <= lw[0] <= rs[0] + 10:\n",
    "        print(\"left swing\")\n",
    "        \n",
    "    return verb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "right swing\n",
      "0: 640x640 2 persons, 410.9ms\n",
      "Speed: 454.4ms preprocess, 410.9ms inference, 2.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "swing back\n",
      "0: 640x640 3 persons, 48.8ms\n",
      "Speed: 7.1ms preprocess, 48.8ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "swing back\n",
      "0: 640x640 2 persons, 48.6ms\n",
      "Speed: 2.4ms preprocess, 48.6ms inference, 3.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "swing back\n",
      "0: 640x640 2 persons, 51.0ms\n",
      "Speed: 1.9ms preprocess, 51.0ms inference, 3.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "swing back\n",
      "0: 640x640 2 persons, 50.6ms\n",
      "Speed: 2.1ms preprocess, 50.6ms inference, 3.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "swing back\n",
      "0: 640x640 2 persons, 51.1ms\n",
      "Speed: 1.9ms preprocess, 51.1ms inference, 3.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "swing back\n",
      "0: 640x640 2 persons, 51.3ms\n",
      "Speed: 2.0ms preprocess, 51.3ms inference, 3.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "swing back\n",
      "0: 640x640 2 persons, 50.7ms\n",
      "Speed: 2.1ms preprocess, 50.7ms inference, 3.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "swing back\n",
      "0: 640x640 2 persons, 51.4ms\n",
      "Speed: 1.8ms preprocess, 51.4ms inference, 2.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "swing back\n",
      "0: 640x640 2 persons, 50.8ms\n",
      "Speed: 2.6ms preprocess, 50.8ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "swing back\n",
      "0: 640x640 2 persons, 52.8ms\n",
      "Speed: 1.9ms preprocess, 52.8ms inference, 3.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "swing back\n",
      "0: 640x640 2 persons, 50.7ms\n",
      "Speed: 1.7ms preprocess, 50.7ms inference, 3.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "swing back\n",
      "0: 640x640 2 persons, 51.2ms\n",
      "Speed: 2.0ms preprocess, 51.2ms inference, 2.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "swing back\n",
      "0: 640x640 2 persons, 50.9ms\n",
      "Speed: 1.9ms preprocess, 50.9ms inference, 2.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "swing back\n",
      "0: 640x640 1 person, 50.9ms\n",
      "Speed: 2.0ms preprocess, 50.9ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "swing back\n",
      "0: 640x640 1 person, 51.6ms\n",
      "Speed: 2.2ms preprocess, 51.6ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "swing back\n",
      "0: 640x640 2 persons, 58.0ms\n",
      "Speed: 3.6ms preprocess, 58.0ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "swing back\n",
      "0: 640x640 2 persons, 47.6ms\n",
      "Speed: 5.8ms preprocess, 47.6ms inference, 3.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "swing back\n",
      "0: 640x640 2 persons, 51.5ms\n",
      "Speed: 2.0ms preprocess, 51.5ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "swing back\n",
      "0: 640x640 2 persons, 53.4ms\n",
      "Speed: 1.5ms preprocess, 53.4ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "swing back\n",
      "0: 640x640 2 persons, 51.4ms\n",
      "Speed: 1.5ms preprocess, 51.4ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "swing back\n",
      "0: 640x640 1 person, 50.6ms\n",
      "Speed: 1.9ms preprocess, 50.6ms inference, 3.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "swing back\n",
      "0: 640x640 1 person, 48.2ms\n",
      "Speed: 1.9ms preprocess, 48.2ms inference, 2.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "swing back\n",
      "0: 640x640 1 person, 51.6ms\n",
      "Speed: 1.6ms preprocess, 51.6ms inference, 3.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "swing back\n",
      "0: 640x640 2 persons, 51.1ms\n",
      "Speed: 1.9ms preprocess, 51.1ms inference, 3.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "swing back\n",
      "0: 640x640 2 persons, 48.7ms\n",
      "Speed: 1.9ms preprocess, 48.7ms inference, 2.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "swing back\n",
      "0: 640x640 2 persons, 51.9ms\n",
      "Speed: 1.6ms preprocess, 51.9ms inference, 3.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "swing back\n",
      "0: 640x640 2 persons, 50.1ms\n",
      "Speed: 1.7ms preprocess, 50.1ms inference, 3.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "swing back\n",
      "0: 640x640 2 persons, 50.9ms\n",
      "Speed: 1.9ms preprocess, 50.9ms inference, 2.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "swing back\n",
      "0: 640x640 2 persons, 183.4ms\n",
      "Speed: 3.0ms preprocess, 183.4ms inference, 3.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "swing back\n",
      "0: 640x640 2 persons, 49.8ms\n",
      "Speed: 2.8ms preprocess, 49.8ms inference, 2.8ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    }
   ],
   "source": [
    "# video path file\n",
    "video_path = r\"/home/vijayvkb98/gitthing/knowledge-graph-for-action-understanding/CKT_DATASET/Bowled/001.mp4\"\n",
    "\n",
    "# open video file\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# set the start and end frame indices\n",
    "start_frame = 0\n",
    "end_frame = 100\n",
    "T = False\n",
    "# Loop through frames\n",
    "for frame_idx in range(start_frame, end_frame):\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    if not ret: break\n",
    "        \n",
    "    rframe = cv2.resize(frame, (640, 640))\n",
    "    # rframe = cv2.resize(frame, (224, 224))\n",
    "    # rframe = cv2.cvtColor(rframe, cv2.COLOR_BGR2RGB)\n",
    "    rframe.flags.writeable = False\n",
    "    \n",
    "    results = model(source=rframe, conf=0.3, stream=True, device='cuda')\n",
    "    \n",
    "    rframe.flags.writeable = True\n",
    "    \n",
    "    for r in results:\n",
    "        img = r.orig_img\n",
    "        \n",
    "        try:\n",
    "            # obtain coordinates of keypoints\n",
    "            keypoints = r.keypoints.xy[0].cpu().numpy()\n",
    "            mark_keypoints(rframe, keypoints)\n",
    "        except:\n",
    "            pass\n",
    "\t\t\n",
    "    cv2.imshow(\"frame\",rframe)\n",
    "    if cv2.waitKey(0) & 0xFF == ord('q'):\n",
    "        break\n",
    "\t\t\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.], device='cuda:0')\n",
      "conf: tensor([0.9044], device='cuda:0')\n",
      "data: tensor([[301.6605,   1.0169, 466.5988, 544.7084,   0.9044,   0.0000]], device='cuda:0')\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (640, 640)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[384.1296, 272.8627, 164.9384, 543.6915]], device='cuda:0')\n",
      "xywhn: tensor([[0.6002, 0.4263, 0.2577, 0.8495]], device='cuda:0')\n",
      "xyxy: tensor([[301.6605,   1.0169, 466.5988, 544.7084]], device='cuda:0')\n",
      "xyxyn: tensor([[0.4713, 0.0016, 0.7291, 0.8511]], device='cuda:0')\n",
      "0: 640x640 1 person, 86.8ms\n",
      "Speed: 2.1ms preprocess, 86.8ms inference, 4.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.], device='cuda:0')\n",
      "conf: tensor([0.8625], device='cuda:0')\n",
      "data: tensor([[301.1427,   0.0000, 450.4072, 543.6735,   0.8625,   0.0000]], device='cuda:0')\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (640, 640)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[375.7749, 271.8368, 149.2645, 543.6735]], device='cuda:0')\n",
      "xywhn: tensor([[0.5871, 0.4247, 0.2332, 0.8495]], device='cuda:0')\n",
      "xyxy: tensor([[301.1427,   0.0000, 450.4072, 543.6735]], device='cuda:0')\n",
      "xyxyn: tensor([[0.4705, 0.0000, 0.7038, 0.8495]], device='cuda:0')\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.], device='cuda:0')\n",
      "conf: tensor([0.6727], device='cuda:0')\n",
      "data: tensor([[580.3801, 471.6272, 631.6604, 639.5398,   0.6727,   0.0000]], device='cuda:0')\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (640, 640)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[606.0203, 555.5835,  51.2803, 167.9126]], device='cuda:0')\n",
      "xywhn: tensor([[0.9469, 0.8681, 0.0801, 0.2624]], device='cuda:0')\n",
      "xyxy: tensor([[580.3801, 471.6272, 631.6604, 639.5398]], device='cuda:0')\n",
      "xyxyn: tensor([[0.9068, 0.7369, 0.9870, 0.9993]], device='cuda:0')\n",
      "0: 640x640 2 persons, 238.2ms\n",
      "Speed: 4.1ms preprocess, 238.2ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.], device='cuda:0')\n",
      "conf: tensor([0.7038], device='cuda:0')\n",
      "data: tensor([[300.1368,   0.0000, 452.9398, 541.5510,   0.7038,   0.0000]], device='cuda:0')\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (640, 640)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[376.5383, 270.7755, 152.8030, 541.5510]], device='cuda:0')\n",
      "xywhn: tensor([[0.5883, 0.4231, 0.2388, 0.8462]], device='cuda:0')\n",
      "xyxy: tensor([[300.1368,   0.0000, 452.9398, 541.5510]], device='cuda:0')\n",
      "xyxyn: tensor([[0.4690, 0.0000, 0.7077, 0.8462]], device='cuda:0')\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.], device='cuda:0')\n",
      "conf: tensor([0.6951], device='cuda:0')\n",
      "data: tensor([[580.3477, 464.6395, 631.6255, 639.5780,   0.6951,   0.0000]], device='cuda:0')\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (640, 640)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[605.9866, 552.1088,  51.2778, 174.9385]], device='cuda:0')\n",
      "xywhn: tensor([[0.9469, 0.8627, 0.0801, 0.2733]], device='cuda:0')\n",
      "xyxy: tensor([[580.3477, 464.6395, 631.6255, 639.5780]], device='cuda:0')\n",
      "xyxyn: tensor([[0.9068, 0.7260, 0.9869, 0.9993]], device='cuda:0')\n",
      "0: 640x640 2 persons, 81.4ms\n",
      "Speed: 2.0ms preprocess, 81.4ms inference, 3.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.], device='cuda:0')\n",
      "conf: tensor([0.6641], device='cuda:0')\n",
      "data: tensor([[580.5085, 462.1011, 631.6318, 639.5963,   0.6641,   0.0000]], device='cuda:0')\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (640, 640)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[606.0702, 550.8487,  51.1233, 177.4951]], device='cuda:0')\n",
      "xywhn: tensor([[0.9470, 0.8607, 0.0799, 0.2773]], device='cuda:0')\n",
      "xyxy: tensor([[580.5085, 462.1011, 631.6318, 639.5963]], device='cuda:0')\n",
      "xyxyn: tensor([[0.9070, 0.7220, 0.9869, 0.9994]], device='cuda:0')\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.], device='cuda:0')\n",
      "conf: tensor([0.6465], device='cuda:0')\n",
      "data: tensor([[301.0770,   0.6682, 451.7254, 541.2424,   0.6465,   0.0000]], device='cuda:0')\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (640, 640)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[376.4012, 270.9553, 150.6484, 540.5742]], device='cuda:0')\n",
      "xywhn: tensor([[0.5881, 0.4234, 0.2354, 0.8446]], device='cuda:0')\n",
      "xyxy: tensor([[301.0770,   0.6682, 451.7254, 541.2424]], device='cuda:0')\n",
      "xyxyn: tensor([[0.4704, 0.0010, 0.7058, 0.8457]], device='cuda:0')\n",
      "0: 640x640 2 persons, 81.8ms\n",
      "Speed: 2.6ms preprocess, 81.8ms inference, 2.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.], device='cuda:0')\n",
      "conf: tensor([0.8586], device='cuda:0')\n",
      "data: tensor([[3.0127e+02, 2.0850e-01, 4.5390e+02, 5.4243e+02, 8.5857e-01, 0.0000e+00]], device='cuda:0')\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (640, 640)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[377.5863, 271.3214, 152.6246, 542.2258]], device='cuda:0')\n",
      "xywhn: tensor([[0.5900, 0.4239, 0.2385, 0.8472]], device='cuda:0')\n",
      "xyxy: tensor([[3.0127e+02, 2.0850e-01, 4.5390e+02, 5.4243e+02]], device='cuda:0')\n",
      "xyxyn: tensor([[4.7074e-01, 3.2578e-04, 7.0922e-01, 8.4755e-01]], device='cuda:0')\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.], device='cuda:0')\n",
      "conf: tensor([0.6291], device='cuda:0')\n",
      "data: tensor([[5.8068e+02, 4.6188e+02, 6.3159e+02, 6.3958e+02, 6.2911e-01, 0.0000e+00]], device='cuda:0')\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (640, 640)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[606.1359, 550.7273,  50.9060, 177.6957]], device='cuda:0')\n",
      "xywhn: tensor([[0.9471, 0.8605, 0.0795, 0.2776]], device='cuda:0')\n",
      "xyxy: tensor([[580.6829, 461.8795, 631.5889, 639.5751]], device='cuda:0')\n",
      "xyxyn: tensor([[0.9073, 0.7217, 0.9869, 0.9993]], device='cuda:0')\n",
      "0: 640x640 2 persons, 97.0ms\n",
      "Speed: 2.5ms preprocess, 97.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.], device='cuda:0')\n",
      "conf: tensor([0.8541], device='cuda:0')\n",
      "data: tensor([[301.1690,   0.0000, 454.3912, 542.2452,   0.8541,   0.0000]], device='cuda:0')\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (640, 640)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[377.7801, 271.1226, 153.2222, 542.2452]], device='cuda:0')\n",
      "xywhn: tensor([[0.5903, 0.4236, 0.2394, 0.8473]], device='cuda:0')\n",
      "xyxy: tensor([[301.1690,   0.0000, 454.3912, 542.2452]], device='cuda:0')\n",
      "xyxyn: tensor([[0.4706, 0.0000, 0.7100, 0.8473]], device='cuda:0')\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.], device='cuda:0')\n",
      "conf: tensor([0.6006], device='cuda:0')\n",
      "data: tensor([[5.8064e+02, 4.6175e+02, 6.3158e+02, 6.3957e+02, 6.0065e-01, 0.0000e+00]], device='cuda:0')\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (640, 640)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[606.1099, 550.6601,  50.9482, 177.8207]], device='cuda:0')\n",
      "xywhn: tensor([[0.9470, 0.8604, 0.0796, 0.2778]], device='cuda:0')\n",
      "xyxy: tensor([[580.6357, 461.7498, 631.5840, 639.5704]], device='cuda:0')\n",
      "xyxyn: tensor([[0.9072, 0.7215, 0.9868, 0.9993]], device='cuda:0')\n",
      "0: 640x640 2 persons, 96.0ms\n",
      "Speed: 2.2ms preprocess, 96.0ms inference, 2.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.], device='cuda:0')\n",
      "conf: tensor([0.9029], device='cuda:0')\n",
      "data: tensor([[3.0177e+02, 8.0688e-02, 4.5038e+02, 5.4311e+02, 9.0292e-01, 0.0000e+00]], device='cuda:0')\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (640, 640)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[376.0762, 271.5949, 148.6174, 543.0285]], device='cuda:0')\n",
      "xywhn: tensor([[0.5876, 0.4244, 0.2322, 0.8485]], device='cuda:0')\n",
      "xyxy: tensor([[3.0177e+02, 8.0688e-02, 4.5038e+02, 5.4311e+02]], device='cuda:0')\n",
      "xyxyn: tensor([[4.7151e-01, 1.2608e-04, 7.0373e-01, 8.4861e-01]], device='cuda:0')\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.], device='cuda:0')\n",
      "conf: tensor([0.6396], device='cuda:0')\n",
      "data: tensor([[580.5414, 460.4873, 631.6443, 639.4563,   0.6396,   0.0000]], device='cuda:0')\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (640, 640)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[606.0929, 549.9718,  51.1029, 178.9690]], device='cuda:0')\n",
      "xywhn: tensor([[0.9470, 0.8593, 0.0798, 0.2796]], device='cuda:0')\n",
      "xyxy: tensor([[580.5414, 460.4873, 631.6443, 639.4563]], device='cuda:0')\n",
      "xyxyn: tensor([[0.9071, 0.7195, 0.9869, 0.9992]], device='cuda:0')\n",
      "0: 640x640 2 persons, 96.4ms\n",
      "Speed: 2.5ms preprocess, 96.4ms inference, 3.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.], device='cuda:0')\n",
      "conf: tensor([0.9064], device='cuda:0')\n",
      "data: tensor([[3.0252e+02, 1.7175e-01, 4.4548e+02, 5.4242e+02, 9.0644e-01, 0.0000e+00]], device='cuda:0')\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (640, 640)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[373.9985, 271.2951, 142.9622, 542.2467]], device='cuda:0')\n",
      "xywhn: tensor([[0.5844, 0.4239, 0.2234, 0.8473]], device='cuda:0')\n",
      "xyxy: tensor([[3.0252e+02, 1.7175e-01, 4.4548e+02, 5.4242e+02]], device='cuda:0')\n",
      "xyxyn: tensor([[4.7268e-01, 2.6836e-04, 6.9606e-01, 8.4753e-01]], device='cuda:0')\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.], device='cuda:0')\n",
      "conf: tensor([0.6413], device='cuda:0')\n",
      "data: tensor([[580.4069, 460.4886, 631.6049, 639.4799,   0.6413,   0.0000]], device='cuda:0')\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (640, 640)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[606.0059, 549.9843,  51.1980, 178.9912]], device='cuda:0')\n",
      "xywhn: tensor([[0.9469, 0.8594, 0.0800, 0.2797]], device='cuda:0')\n",
      "xyxy: tensor([[580.4069, 460.4886, 631.6049, 639.4799]], device='cuda:0')\n",
      "xyxyn: tensor([[0.9069, 0.7195, 0.9869, 0.9992]], device='cuda:0')\n",
      "0: 640x640 2 persons, 95.4ms\n",
      "Speed: 2.1ms preprocess, 95.4ms inference, 3.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.], device='cuda:0')\n",
      "conf: tensor([0.8985], device='cuda:0')\n",
      "data: tensor([[3.0220e+02, 4.0588e-02, 4.4363e+02, 5.4262e+02, 8.9851e-01, 0.0000e+00]], device='cuda:0')\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (640, 640)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[372.9173, 271.3295, 141.4330, 542.5779]], device='cuda:0')\n",
      "xywhn: tensor([[0.5827, 0.4240, 0.2210, 0.8478]], device='cuda:0')\n",
      "xyxy: tensor([[3.0220e+02, 4.0588e-02, 4.4363e+02, 5.4262e+02]], device='cuda:0')\n",
      "xyxyn: tensor([[4.7219e-01, 6.3419e-05, 6.9318e-01, 8.4784e-01]], device='cuda:0')\n",
      "0: 640x640 1 person, 95.6ms\n",
      "Speed: 2.4ms preprocess, 95.6ms inference, 4.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.], device='cuda:0')\n",
      "conf: tensor([0.8824], device='cuda:0')\n",
      "data: tensor([[3.0226e+02, 4.2548e-01, 4.4346e+02, 5.4341e+02, 8.8243e-01, 0.0000e+00]], device='cuda:0')\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (640, 640)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[372.8647, 271.9184, 141.2003, 542.9859]], device='cuda:0')\n",
      "xywhn: tensor([[0.5826, 0.4249, 0.2206, 0.8484]], device='cuda:0')\n",
      "xyxy: tensor([[3.0226e+02, 4.2548e-01, 4.4346e+02, 5.4341e+02]], device='cuda:0')\n",
      "xyxyn: tensor([[4.7229e-01, 6.6481e-04, 6.9291e-01, 8.4908e-01]], device='cuda:0')\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.], device='cuda:0')\n",
      "conf: tensor([0.6048], device='cuda:0')\n",
      "data: tensor([[5.8144e+02, 4.6289e+02, 6.3175e+02, 6.3933e+02, 6.0482e-01, 0.0000e+00]], device='cuda:0')\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (640, 640)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[606.5945, 551.1122,  50.3081, 176.4355]], device='cuda:0')\n",
      "xywhn: tensor([[0.9478, 0.8611, 0.0786, 0.2757]], device='cuda:0')\n",
      "xyxy: tensor([[581.4404, 462.8945, 631.7485, 639.3300]], device='cuda:0')\n",
      "xyxyn: tensor([[0.9085, 0.7233, 0.9871, 0.9990]], device='cuda:0')\n",
      "0: 640x640 2 persons, 79.5ms\n",
      "Speed: 1.7ms preprocess, 79.5ms inference, 2.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.], device='cuda:0')\n",
      "conf: tensor([0.9048], device='cuda:0')\n",
      "data: tensor([[302.0909,   0.5551, 442.9865, 545.4244,   0.9048,   0.0000]], device='cuda:0')\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (640, 640)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[372.5387, 272.9897, 140.8956, 544.8694]], device='cuda:0')\n",
      "xywhn: tensor([[0.5821, 0.4265, 0.2201, 0.8514]], device='cuda:0')\n",
      "xyxy: tensor([[302.0909,   0.5551, 442.9865, 545.4244]], device='cuda:0')\n",
      "xyxyn: tensor([[0.4720, 0.0009, 0.6922, 0.8522]], device='cuda:0')\n",
      "0: 640x640 1 person, 99.4ms\n",
      "Speed: 2.5ms preprocess, 99.4ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.], device='cuda:0')\n",
      "conf: tensor([0.9024], device='cuda:0')\n",
      "data: tensor([[302.1500,   0.7277, 442.9540, 545.5544,   0.9024,   0.0000]], device='cuda:0')\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (640, 640)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[372.5520, 273.1411, 140.8040, 544.8267]], device='cuda:0')\n",
      "xywhn: tensor([[0.5821, 0.4268, 0.2200, 0.8513]], device='cuda:0')\n",
      "xyxy: tensor([[302.1500,   0.7277, 442.9540, 545.5544]], device='cuda:0')\n",
      "xyxyn: tensor([[0.4721, 0.0011, 0.6921, 0.8524]], device='cuda:0')\n",
      "0: 640x640 1 person, 80.4ms\n",
      "Speed: 1.8ms preprocess, 80.4ms inference, 2.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.], device='cuda:0')\n",
      "conf: tensor([0.9173], device='cuda:0')\n",
      "data: tensor([[3.0176e+02, 3.4308e-01, 4.4187e+02, 5.4675e+02, 9.1730e-01, 0.0000e+00]], device='cuda:0')\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (640, 640)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[371.8142, 273.5471, 140.1140, 546.4081]], device='cuda:0')\n",
      "xywhn: tensor([[0.5810, 0.4274, 0.2189, 0.8538]], device='cuda:0')\n",
      "xyxy: tensor([[3.0176e+02, 3.4308e-01, 4.4187e+02, 5.4675e+02]], device='cuda:0')\n",
      "xyxyn: tensor([[4.7150e-01, 5.3606e-04, 6.9042e-01, 8.5430e-01]], device='cuda:0')\n",
      "0: 640x640 1 person, 78.7ms\n",
      "Speed: 2.0ms preprocess, 78.7ms inference, 2.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.], device='cuda:0')\n",
      "conf: tensor([0.7569], device='cuda:0')\n",
      "data: tensor([[101.3855,   0.7603, 438.9634, 547.5623,   0.7569,   0.0000]], device='cuda:0')\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (640, 640)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[270.1745, 274.1613, 337.5779, 546.8020]], device='cuda:0')\n",
      "xywhn: tensor([[0.4221, 0.4284, 0.5275, 0.8544]], device='cuda:0')\n",
      "xyxy: tensor([[101.3855,   0.7603, 438.9634, 547.5623]], device='cuda:0')\n",
      "xyxyn: tensor([[0.1584, 0.0012, 0.6859, 0.8556]], device='cuda:0')\n",
      "0: 640x640 1 person, 97.4ms\n",
      "Speed: 1.6ms preprocess, 97.4ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.], device='cuda:0')\n",
      "conf: tensor([0.9003], device='cuda:0')\n",
      "data: tensor([[300.2493,   0.0000, 435.9052, 548.2308,   0.9003,   0.0000]], device='cuda:0')\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (640, 640)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[368.0772, 274.1154, 135.6559, 548.2308]], device='cuda:0')\n",
      "xywhn: tensor([[0.5751, 0.4283, 0.2120, 0.8566]], device='cuda:0')\n",
      "xyxy: tensor([[300.2493,   0.0000, 435.9052, 548.2308]], device='cuda:0')\n",
      "xyxyn: tensor([[0.4691, 0.0000, 0.6811, 0.8566]], device='cuda:0')\n",
      "0: 640x640 1 person, 80.4ms\n",
      "Speed: 1.7ms preprocess, 80.4ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.], device='cuda:0')\n",
      "conf: tensor([0.7984], device='cuda:0')\n",
      "data: tensor([[297.8066,   0.6605, 434.0944, 549.7971,   0.7984,   0.0000]], device='cuda:0')\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (640, 640)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[365.9505, 275.2288, 136.2878, 549.1366]], device='cuda:0')\n",
      "xywhn: tensor([[0.5718, 0.4300, 0.2129, 0.8580]], device='cuda:0')\n",
      "xyxy: tensor([[297.8066,   0.6605, 434.0944, 549.7971]], device='cuda:0')\n",
      "xyxyn: tensor([[0.4653, 0.0010, 0.6783, 0.8591]], device='cuda:0')\n",
      "0: 640x640 1 person, 79.1ms\n",
      "Speed: 1.5ms preprocess, 79.1ms inference, 2.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.], device='cuda:0')\n",
      "conf: tensor([0.7397], device='cuda:0')\n",
      "data: tensor([[124.8317,   0.8683, 433.9278, 539.2450,   0.7397,   0.0000]], device='cuda:0')\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (640, 640)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[279.3798, 270.0567, 309.0961, 538.3766]], device='cuda:0')\n",
      "xywhn: tensor([[0.4365, 0.4220, 0.4830, 0.8412]], device='cuda:0')\n",
      "xyxy: tensor([[124.8317,   0.8683, 433.9278, 539.2450]], device='cuda:0')\n",
      "xyxyn: tensor([[0.1950, 0.0014, 0.6780, 0.8426]], device='cuda:0')\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.], device='cuda:0')\n",
      "conf: tensor([0.6036], device='cuda:0')\n",
      "data: tensor([[5.8792e+02, 5.5569e+02, 6.3166e+02, 6.3958e+02, 6.0357e-01, 0.0000e+00]], device='cuda:0')\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (640, 640)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[609.7906, 597.6335,  43.7452,  83.8932]], device='cuda:0')\n",
      "xywhn: tensor([[0.9528, 0.9338, 0.0684, 0.1311]], device='cuda:0')\n",
      "xyxy: tensor([[587.9180, 555.6870, 631.6632, 639.5801]], device='cuda:0')\n",
      "xyxyn: tensor([[0.9186, 0.8683, 0.9870, 0.9993]], device='cuda:0')\n",
      "0: 640x640 2 persons, 80.9ms\n",
      "Speed: 1.8ms preprocess, 80.9ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.], device='cuda:0')\n",
      "conf: tensor([0.8163], device='cuda:0')\n",
      "data: tensor([[1.2190e+02, 4.0570e-01, 4.3579e+02, 5.3051e+02, 8.1635e-01, 0.0000e+00]], device='cuda:0')\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (640, 640)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[278.8405, 265.4557, 313.8890, 530.0999]], device='cuda:0')\n",
      "xywhn: tensor([[0.4357, 0.4148, 0.4905, 0.8283]], device='cuda:0')\n",
      "xyxy: tensor([[1.2190e+02, 4.0570e-01, 4.3579e+02, 5.3051e+02]], device='cuda:0')\n",
      "xyxyn: tensor([[1.9046e-01, 6.3391e-04, 6.8091e-01, 8.2892e-01]], device='cuda:0')\n",
      "0: 640x640 1 person, 66.1ms\n",
      "Speed: 2.4ms preprocess, 66.1ms inference, 3.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 87.2ms\n",
      "Speed: 2.5ms preprocess, 87.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.], device='cuda:0')\n",
      "conf: tensor([0.8973], device='cuda:0')\n",
      "data: tensor([[150.1562,   0.7880, 436.9455, 541.0808,   0.8973,   0.0000]], device='cuda:0')\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (640, 640)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[293.5509, 270.9344, 286.7893, 540.2928]], device='cuda:0')\n",
      "xywhn: tensor([[0.4587, 0.4233, 0.4481, 0.8442]], device='cuda:0')\n",
      "xyxy: tensor([[150.1562,   0.7880, 436.9455, 541.0808]], device='cuda:0')\n",
      "xyxyn: tensor([[0.2346, 0.0012, 0.6827, 0.8454]], device='cuda:0')\n",
      "0: 640x640 1 person, 83.2ms\n",
      "Speed: 2.5ms preprocess, 83.2ms inference, 2.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.], device='cuda:0')\n",
      "conf: tensor([0.9222], device='cuda:0')\n",
      "data: tensor([[1.6198e+02, 1.2830e-01, 4.3370e+02, 5.4352e+02, 9.2216e-01, 0.0000e+00]], device='cuda:0')\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (640, 640)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[297.8427, 271.8222, 271.7196, 543.3878]], device='cuda:0')\n",
      "xywhn: tensor([[0.4654, 0.4247, 0.4246, 0.8490]], device='cuda:0')\n",
      "xyxy: tensor([[1.6198e+02, 1.2830e-01, 4.3370e+02, 5.4352e+02]], device='cuda:0')\n",
      "xyxyn: tensor([[2.5310e-01, 2.0046e-04, 6.7766e-01, 8.4924e-01]], device='cuda:0')\n",
      "0: 640x640 1 person, 81.3ms\n",
      "Speed: 1.9ms preprocess, 81.3ms inference, 4.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.], device='cuda:0')\n",
      "conf: tensor([0.8313], device='cuda:0')\n",
      "data: tensor([[171.0219,   0.0000, 499.7187, 545.6490,   0.8313,   0.0000]], device='cuda:0')\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (640, 640)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[335.3703, 272.8245, 328.6968, 545.6490]], device='cuda:0')\n",
      "xywhn: tensor([[0.5240, 0.4263, 0.5136, 0.8526]], device='cuda:0')\n",
      "xyxy: tensor([[171.0219,   0.0000, 499.7187, 545.6490]], device='cuda:0')\n",
      "xyxyn: tensor([[0.2672, 0.0000, 0.7808, 0.8526]], device='cuda:0')\n",
      "0: 640x640 1 person, 121.6ms\n",
      "Speed: 1.5ms preprocess, 121.6ms inference, 2.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.], device='cuda:0')\n",
      "conf: tensor([0.7414], device='cuda:0')\n",
      "data: tensor([[1.8074e+02, 1.6003e-01, 4.3110e+02, 5.3981e+02, 7.4144e-01, 0.0000e+00]], device='cuda:0')\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (640, 640)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[305.9209, 269.9862, 250.3678, 539.6523]], device='cuda:0')\n",
      "xywhn: tensor([[0.4780, 0.4219, 0.3912, 0.8432]], device='cuda:0')\n",
      "xyxy: tensor([[1.8074e+02, 1.6003e-01, 4.3110e+02, 5.3981e+02]], device='cuda:0')\n",
      "xyxyn: tensor([[2.8240e-01, 2.5005e-04, 6.7360e-01, 8.4346e-01]], device='cuda:0')\n",
      "0: 640x640 1 person, 95.0ms\n",
      "Speed: 2.0ms preprocess, 95.0ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.], device='cuda:0')\n",
      "conf: tensor([0.7588], device='cuda:0')\n",
      "data: tensor([[1.8065e+02, 1.6003e-01, 4.3115e+02, 5.4055e+02, 7.5884e-01, 0.0000e+00]], device='cuda:0')\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (640, 640)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[305.9003, 270.3569, 250.5061, 540.3937]], device='cuda:0')\n",
      "xywhn: tensor([[0.4780, 0.4224, 0.3914, 0.8444]], device='cuda:0')\n",
      "xyxy: tensor([[1.8065e+02, 1.6003e-01, 4.3115e+02, 5.4055e+02]], device='cuda:0')\n",
      "xyxyn: tensor([[2.8226e-01, 2.5005e-04, 6.7368e-01, 8.4462e-01]], device='cuda:0')\n",
      "0: 640x640 1 person, 116.6ms\n",
      "Speed: 2.6ms preprocess, 116.6ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.], device='cuda:0')\n",
      "conf: tensor([0.7281], device='cuda:0')\n",
      "data: tensor([[268.2767,   0.0000, 433.0477, 544.0219,   0.7281,   0.0000]], device='cuda:0')\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (640, 640)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[350.6622, 272.0109, 164.7710, 544.0219]], device='cuda:0')\n",
      "xywhn: tensor([[0.5479, 0.4250, 0.2575, 0.8500]], device='cuda:0')\n",
      "xyxy: tensor([[268.2767,   0.0000, 433.0477, 544.0219]], device='cuda:0')\n",
      "xyxyn: tensor([[0.4192, 0.0000, 0.6766, 0.8500]], device='cuda:0')\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.], device='cuda:0')\n",
      "conf: tensor([0.6347], device='cuda:0')\n",
      "data: tensor([[354.4121,   9.7463, 537.8793, 523.7316,   0.6347,   0.0000]], device='cuda:0')\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (640, 640)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[446.1457, 266.7390, 183.4672, 513.9854]], device='cuda:0')\n",
      "xywhn: tensor([[0.6971, 0.4168, 0.2867, 0.8031]], device='cuda:0')\n",
      "xyxy: tensor([[354.4121,   9.7463, 537.8793, 523.7316]], device='cuda:0')\n",
      "xyxyn: tensor([[0.5538, 0.0152, 0.8404, 0.8183]], device='cuda:0')\n",
      "0: 640x640 2 persons, 105.0ms\n",
      "Speed: 2.6ms preprocess, 105.0ms inference, 3.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.], device='cuda:0')\n",
      "conf: tensor([0.8430], device='cuda:0')\n",
      "data: tensor([[263.4923,   0.0000, 434.0343, 553.8116,   0.8430,   0.0000]], device='cuda:0')\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (640, 640)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[348.7633, 276.9058, 170.5420, 553.8116]], device='cuda:0')\n",
      "xywhn: tensor([[0.5449, 0.4327, 0.2665, 0.8653]], device='cuda:0')\n",
      "xyxy: tensor([[263.4923,   0.0000, 434.0343, 553.8116]], device='cuda:0')\n",
      "xyxyn: tensor([[0.4117, 0.0000, 0.6782, 0.8653]], device='cuda:0')\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.], device='cuda:0')\n",
      "conf: tensor([0.7491], device='cuda:0')\n",
      "data: tensor([[379.8685,   9.5379, 525.6711, 501.3070,   0.7491,   0.0000]], device='cuda:0')\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (640, 640)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[452.7698, 255.4225, 145.8026, 491.7691]], device='cuda:0')\n",
      "xywhn: tensor([[0.7075, 0.3991, 0.2278, 0.7684]], device='cuda:0')\n",
      "xyxy: tensor([[379.8685,   9.5379, 525.6711, 501.3070]], device='cuda:0')\n",
      "xyxyn: tensor([[0.5935, 0.0149, 0.8214, 0.7833]], device='cuda:0')\n",
      "0: 640x640 2 persons, 82.4ms\n",
      "Speed: 1.9ms preprocess, 82.4ms inference, 2.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.], device='cuda:0')\n",
      "conf: tensor([0.8658], device='cuda:0')\n",
      "data: tensor([[2.6962e+02, 2.0270e-01, 4.2911e+02, 5.5736e+02, 8.6581e-01, 0.0000e+00]], device='cuda:0')\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (640, 640)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[349.3655, 278.7815, 159.4958, 557.1577]], device='cuda:0')\n",
      "xywhn: tensor([[0.5459, 0.4356, 0.2492, 0.8706]], device='cuda:0')\n",
      "xyxy: tensor([[2.6962e+02, 2.0270e-01, 4.2911e+02, 5.5736e+02]], device='cuda:0')\n",
      "xyxyn: tensor([[4.2128e-01, 3.1672e-04, 6.7049e-01, 8.7088e-01]], device='cuda:0')\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.], device='cuda:0')\n",
      "conf: tensor([0.8146], device='cuda:0')\n",
      "data: tensor([[379.9380,   1.7467, 534.4391, 506.6570,   0.8146,   0.0000]], device='cuda:0')\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (640, 640)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[457.1885, 254.2018, 154.5011, 504.9103]], device='cuda:0')\n",
      "xywhn: tensor([[0.7144, 0.3972, 0.2414, 0.7889]], device='cuda:0')\n",
      "xyxy: tensor([[379.9380,   1.7467, 534.4391, 506.6570]], device='cuda:0')\n",
      "xyxyn: tensor([[0.5937, 0.0027, 0.8351, 0.7917]], device='cuda:0')\n",
      "0: 640x640 2 persons, 82.3ms\n",
      "Speed: 1.9ms preprocess, 82.3ms inference, 3.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.], device='cuda:0')\n",
      "conf: tensor([0.8621], device='cuda:0')\n",
      "data: tensor([[273.8654,   1.7635, 422.9214, 559.2805,   0.8621,   0.0000]], device='cuda:0')\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (640, 640)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[348.3934, 280.5220, 149.0560, 557.5170]], device='cuda:0')\n",
      "xywhn: tensor([[0.5444, 0.4383, 0.2329, 0.8711]], device='cuda:0')\n",
      "xyxy: tensor([[273.8654,   1.7635, 422.9214, 559.2805]], device='cuda:0')\n",
      "xyxyn: tensor([[0.4279, 0.0028, 0.6608, 0.8739]], device='cuda:0')\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.], device='cuda:0')\n",
      "conf: tensor([0.8225], device='cuda:0')\n",
      "data: tensor([[406.0393,  12.1338, 530.3008, 511.1363,   0.8225,   0.0000]], device='cuda:0')\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (640, 640)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[468.1700, 261.6351, 124.2615, 499.0025]], device='cuda:0')\n",
      "xywhn: tensor([[0.7315, 0.4088, 0.1942, 0.7797]], device='cuda:0')\n",
      "xyxy: tensor([[406.0393,  12.1338, 530.3008, 511.1363]], device='cuda:0')\n",
      "xyxyn: tensor([[0.6344, 0.0190, 0.8286, 0.7987]], device='cuda:0')\n",
      "0: 640x640 2 persons, 80.7ms\n",
      "Speed: 1.8ms preprocess, 80.7ms inference, 2.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.], device='cuda:0')\n",
      "conf: tensor([0.8891], device='cuda:0')\n",
      "data: tensor([[2.7876e+02, 1.7334e-02, 4.2607e+02, 5.7452e+02, 8.8906e-01, 0.0000e+00]], device='cuda:0')\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (640, 640)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[352.4156, 287.2691, 147.3011, 574.5035]], device='cuda:0')\n",
      "xywhn: tensor([[0.5506, 0.4489, 0.2302, 0.8977]], device='cuda:0')\n",
      "xyxy: tensor([[2.7876e+02, 1.7334e-02, 4.2607e+02, 5.7452e+02]], device='cuda:0')\n",
      "xyxyn: tensor([[4.3557e-01, 2.7084e-05, 6.6573e-01, 8.9769e-01]], device='cuda:0')\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.], device='cuda:0')\n",
      "conf: tensor([0.8749], device='cuda:0')\n",
      "data: tensor([[390.4800,   9.8123, 530.6736, 517.9032,   0.8749,   0.0000]], device='cuda:0')\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (640, 640)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[460.5768, 263.8577, 140.1937, 508.0909]], device='cuda:0')\n",
      "xywhn: tensor([[0.7197, 0.4123, 0.2191, 0.7939]], device='cuda:0')\n",
      "xyxy: tensor([[390.4800,   9.8123, 530.6736, 517.9032]], device='cuda:0')\n",
      "xyxyn: tensor([[0.6101, 0.0153, 0.8292, 0.8092]], device='cuda:0')\n",
      "0: 640x640 2 persons, 82.1ms\n",
      "Speed: 2.9ms preprocess, 82.1ms inference, 4.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.], device='cuda:0')\n",
      "conf: tensor([0.8938], device='cuda:0')\n",
      "data: tensor([[2.7876e+02, 2.3865e-02, 4.2590e+02, 5.7412e+02, 8.9384e-01, 0.0000e+00]], device='cuda:0')\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (640, 640)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[352.3326, 287.0697, 147.1378, 574.0917]], device='cuda:0')\n",
      "xywhn: tensor([[0.5505, 0.4485, 0.2299, 0.8970]], device='cuda:0')\n",
      "xyxy: tensor([[2.7876e+02, 2.3865e-02, 4.2590e+02, 5.7412e+02]], device='cuda:0')\n",
      "xyxyn: tensor([[4.3557e-01, 3.7289e-05, 6.6547e-01, 8.9706e-01]], device='cuda:0')\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.], device='cuda:0')\n",
      "conf: tensor([0.8847], device='cuda:0')\n",
      "data: tensor([[389.1857,  10.5789, 530.6250, 518.7163,   0.8847,   0.0000]], device='cuda:0')\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (640, 640)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[459.9054, 264.6476, 141.4393, 508.1374]], device='cuda:0')\n",
      "xywhn: tensor([[0.7186, 0.4135, 0.2210, 0.7940]], device='cuda:0')\n",
      "xyxy: tensor([[389.1857,  10.5789, 530.6250, 518.7163]], device='cuda:0')\n",
      "xyxyn: tensor([[0.6081, 0.0165, 0.8291, 0.8105]], device='cuda:0')\n",
      "0: 640x640 2 persons, 81.4ms\n",
      "Speed: 1.6ms preprocess, 81.4ms inference, 2.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.], device='cuda:0')\n",
      "conf: tensor([0.8338], device='cuda:0')\n",
      "data: tensor([[290.3775,   0.6647, 434.9205, 575.8561,   0.8338,   0.0000]], device='cuda:0')\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (640, 640)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[362.6490, 288.2604, 144.5430, 575.1914]], device='cuda:0')\n",
      "xywhn: tensor([[0.5666, 0.4504, 0.2258, 0.8987]], device='cuda:0')\n",
      "xyxy: tensor([[290.3775,   0.6647, 434.9205, 575.8561]], device='cuda:0')\n",
      "xyxyn: tensor([[0.4537, 0.0010, 0.6796, 0.8998]], device='cuda:0')\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.], device='cuda:0')\n",
      "conf: tensor([0.7985], device='cuda:0')\n",
      "data: tensor([[389.2527,   9.3557, 534.1790, 527.6967,   0.7985,   0.0000]], device='cuda:0')\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (640, 640)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[461.7159, 268.5262, 144.9263, 518.3410]], device='cuda:0')\n",
      "xywhn: tensor([[0.7214, 0.4196, 0.2264, 0.8099]], device='cuda:0')\n",
      "xyxy: tensor([[389.2527,   9.3557, 534.1790, 527.6967]], device='cuda:0')\n",
      "xyxyn: tensor([[0.6082, 0.0146, 0.8347, 0.8245]], device='cuda:0')\n",
      "0: 640x640 2 persons, 83.0ms\n",
      "Speed: 1.7ms preprocess, 83.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    }
   ],
   "source": [
    "model = YOLO(\"yolov8x.pt\").to(device)\n",
    "\n",
    "# video path file\n",
    "video_path = r\"/home/vijayvkb98/gitthing/knowledge-graph-for-action-understanding/CKT_DATASET/Bowled/001.mp4\"\n",
    "\n",
    "# open video file\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# set the start and end frame indices\n",
    "start_frame = 0\n",
    "end_frame = 100\n",
    "\n",
    "# Loop through frames\n",
    "# for frame_idx in range(start_frame, end_frame):\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    if not ret: break\n",
    "        \n",
    "    rframe = cv2.resize(frame, (640, 640))    \n",
    "    results = model(source=rframe, conf=0.6, stream=True, device='cuda')\n",
    "\n",
    "    for r in results:\n",
    "        img = r.orig_img\n",
    "        boxes = r.boxes\n",
    "        # print(len(boxes))\n",
    "        for box in boxes:\n",
    "            print(f\"{box[0]}\")\n",
    "            x, y, x1, y1 = box.xyxy[0].cpu().numpy().astype(int)\n",
    "            cv2.rectangle(img, (x, y), (x1, y1), (0, 255, 0), 2)\n",
    "            cv2.putText(rframe, f\"person\", (x+10,y+10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0,0), 2)\n",
    "            cv2.imshow(\"frame\",img)\n",
    "    if cv2.waitKey(0) & 0xFF == ord('q'):\n",
    "        break\n",
    "    \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = YOLO(\"yolov8x-pose-p6.pt\").to(device)\n",
    "model2 = YOLO(\"yolov8x.pt\").to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from ultralytics import YOLO\n",
    "from matplotlib import pyplot as plt\n",
    "import cv2\n",
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Load the YOLO model\n",
    "model = YOLO(\"yolov5s.pt\")\n",
    "\n",
    "# Set the path to the image or video file\n",
    "file_path = \"path/to/image_or_video\"\n",
    "\n",
    "# Perform object detection\n",
    "results = model(file_path)\n",
    "\n",
    "# Display the results\n",
    "results.show()\n",
    "\n",
    "# Save the results\n",
    "results.save()\n",
    "\n",
    "# Access the detected objects\n",
    "objects = results.pandas().xyxy[0]\n",
    "\n",
    "# Print the detected objects\n",
    "# Load the YOLO model\n",
    "model = YOLO(\"yolov5s.pt\")\n",
    "\n",
    "# Set the path to the video file\n",
    "video_path = \"path/to/video\"\n",
    "\n",
    "# Open the video file\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Create a VideoWriter object to save the output video\n",
    "output_path = \"path/to/output_video\"\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "out = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n",
    "\n",
    "# Process each frame of the video\n",
    "while True:\n",
    "    # Read a frame from the video\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    # Break the loop if the video has ended\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    # Perform object detection on the frame\n",
    "    results = model(frame)\n",
    "    \n",
    "    # Draw bounding boxes and labels on the frame\n",
    "    results.render()\n",
    "    \n",
    "    # Get the annotated frame\n",
    "    annotated_frame = results.imgs[0]\n",
    "    \n",
    "    # Write the annotated frame to the output video\n",
    "    out.write(annotated_frame)\n",
    "    \n",
    "    # Display the annotated frame\n",
    "    cv2.imshow(\"Object Detection\", annotated_frame)\n",
    "    \n",
    "    # Break the loop if 'q' is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the video capture and writer objects\n",
    "cap.release()\n",
    "out.release()\n",
    "\n",
    "# Close all OpenCV windows\n",
    "cv2.destroyAllWindows()\n",
    "print(objects)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
